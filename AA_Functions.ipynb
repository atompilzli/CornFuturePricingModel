{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section all functions are defined for training the algorithms. <br>\n",
    "The algorithms used are: <br>\n",
    "1. Persistence Model / NaÃ¯ve Forecast\n",
    "2. Linear Regression\n",
    "3. Support Vector Regressor (SVR)\n",
    "4. Random Forest Regressor\n",
    "5. Long Short Term Memory (LSTM) --> RNN architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_agb_arranger(data, name):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. data: Data set that shall be rearranged\n",
    "        2. name: Name of the column to be rearranged\n",
    "    Returns: Arranged dataset\n",
    "    \"\"\"\n",
    "    # Rearrange data as 1 column\n",
    "    df = pd.concat([data, data.unstack().rename('%s' % (name))\n",
    "                    .reset_index(drop=True)], axis = 1)\n",
    "    df = df[['%s' % (name)]]\n",
    "    df.dropna(inplace=True)\n",
    "    # Add date column as index from 2001-01-01 till 2018-12-31\n",
    "    start = datetime.datetime.strptime(\"2001-01-01\", \"%Y-%m-%d\")\n",
    "    end = datetime.datetime.strptime(\"2019-01-01\", \"%Y-%m-%d\")\n",
    "    date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]\n",
    "    dates = pd.DataFrame(date_generated)\n",
    "    # Concat dates with data and set date as index\n",
    "    arranged = pd.concat([df.reset_index(drop=True), dates.reset_index(drop=True)], \n",
    "                       axis= 1)\n",
    "    arranged.rename(columns={0:'Date'}, inplace=True)\n",
    "    arranged.set_index('Date', inplace=True)\n",
    "    return arranged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for all algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset and set date as index\n",
    "def read_dataframe(file):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. file: File path that already is in correct shape\n",
    "    Returns: Pandas Dataframe with date as index\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    # Transform Date column to datetime\n",
    "    df['Date'] = pd.to_datetime(df.Date)\n",
    "    # Set date column as index\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes multivariate time series and frames it as a supervised learning dataset.\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. data: sequence of observations as a list of 2D Numpy array,\n",
    "        2. n_in: Number of lag observations as input(X).\n",
    "        3. n_out: Number of oberservations as output(y).\n",
    "        4. dropnan: Boolean whether or not to drop rows with NaN values. Defaults to True.\n",
    "    Returns: Pandas Dataframe of series framed for supervised learning\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform series into train and test sets for supervised learning and difference the data\n",
    "def prepare_data(df, n_test, n_features, n_lag, n_seq):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. df: dataframe for data preparation,\n",
    "        2. n_test: size of test set as an absolute number\n",
    "        3. n_features: number of different input features\n",
    "        4. n_lag: number of lag days that are used for forecasting (=lag_days)\n",
    "        5. n_seq: number of forecasting days (=forecast_period)\n",
    "    Returns: \n",
    "        1. scaler: the scaler used (e.g. MinMaxScaler)\n",
    "        2. train: train dataset (scaled and differenced)\n",
    "        3. test: test dataset (scaled and differenced)\n",
    "        4. reframed: the reframed Pandas dataframe for debugging (scaled and differenced)\n",
    "    \"\"\"\n",
    "    # transform data to be stationary by differencing\n",
    "    df_diff = df.diff()\n",
    "    df_diff.dropna(inplace=True)\n",
    "    df_diff = df_diff.values\n",
    "    # transform into supervised learning problem X, y\n",
    "    reframed = series_to_supervised(df_diff, n_lag, n_seq)\n",
    "    # Dropping the forecasting columns we don't want to predict\n",
    "    reframed.drop(reframed.columns[-n_features:],axis=1, inplace=True)\n",
    "    for i in range(1,n_seq):\n",
    "        reframed.drop(reframed.columns[-n_features-i:-i],axis=1, inplace=True)\n",
    "    # Dropping past values of price\n",
    "    reframed.drop(reframed.columns[:1],axis=1, inplace=True)\n",
    "    for i in range(1, lag_days):\n",
    "        reframed.drop(reframed.columns[n_features*i:n_features*i+1],axis=1, inplace=True)\n",
    "    # Get values for scaling\n",
    "    reframed_values = reframed.values\n",
    "    # rescale values to 0, 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_values = scaler.fit_transform(reframed_values)\n",
    "    # split into train and test sets\n",
    "    train, test = scaled_values[:-n_test], scaled_values[-n_test:]\n",
    "    return scaler, train, test, reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons, n_features, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        1. train: train dataset (scaled and differenced)\n",
    "        2. n_lag: number of lag days that are used for forecasting (=lag_days)\n",
    "        3. n_seq: number of forecasting days (=forecast_period)\n",
    "        4. n_batch: batch size used for model fitting\n",
    "        5. n_epochs: number of epochs used in model fitting\n",
    "        6. n_neurons: number of LSTM units in the RNN model\n",
    "        7. n_features: number of independent variables used in the model\n",
    "        8. learning_rate: learning rate of the neural network\n",
    "    Returns: \n",
    "        1. model: model used for training data\n",
    "        2. model_fit: fitted model\n",
    "    \"\"\"\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X_train, y_train = train[:, :-n_seq], train[:, -n_seq:]\n",
    "    X_train = X_train.reshape(X_train.shape[0], n_lag, n_features)\n",
    "    X_test, y_test = test[:, :-n_seq], test[:, -n_seq:]\n",
    "    X_test = X_test.reshape(X_test.shape[0],  n_lag, n_features)\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = n_neurons, input_shape=(X_train.shape[1], X_train.shape[2]), \n",
    "                   return_sequences=True))\n",
    "    model.add(LSTM(units = n_neurons, return_sequences=False))\n",
    "    model.add(Dense(n_seq))\n",
    "    adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "    model_fit = model.fit(X_train, y_train, epochs=n_epochs, batch_size=n_batch, verbose=1, \n",
    "              shuffle=False) # optional: add validation_data=(X_test,y_test) and test as param\n",
    "    return model, model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make forecasts with the trained model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq, n_features):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. model: model used for training\n",
    "        2. n_batch: batch size used for model fitting\n",
    "        3. train: train dataset (scaled and differenced)\n",
    "        4. test: test dataset (scaled and differenced)\n",
    "        5. n_lag: number of lag days that are used for forecasting (=lag_days)\n",
    "        6. n_seq: number of forecasting days (=forecast_period)\n",
    "        7. n_features: number of independent variables used in the model\n",
    "    Returns: List of forecasts for whole test data\n",
    "    \"\"\"\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X_test, y_test = test[i, :(n_features*n_lag)], test[i, -n_seq:]\n",
    "        # reshape input pattern to [1, lag_timesteps, features]\n",
    "        X_test = X_test.reshape(1, n_lag, n_features)\n",
    "        # make forecast\n",
    "        forecast = model.predict(X_test, batch_size=n_batch)\n",
    "        # convert to array\n",
    "        forecast = [x for x in forecast[0, :]]\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. last_ob: Last observation (unscaled and undifferenced)\n",
    "        2. forecast: forecasts for whole test data (unscaled)\n",
    "    Returns: undifferenced list of one forecast series\n",
    "    \"\"\"\n",
    "    # invert first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob[0])\n",
    "    # propagate difference forecast using inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse data transform on forecasts\n",
    "def inverse_transform(df, forecasts, train, test, scaler, n_test, n_seq, \n",
    "                      n_lag, n_features):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. df: Pandas dataframe for debugging (unscaled and undifferenced)\n",
    "        2. forecasts: List of forecasts for whole test data\n",
    "        3. scaler: the scaler used (e.g. MinMaxScaler)\n",
    "        4. n_test: size of test set as an absolute number\n",
    "    Returns: unscaled and undifference forecasts series\n",
    "    \"\"\"\n",
    "    # X, y split\n",
    "    X_test, y_test = test[:, :-n_seq], test[:, -n_seq:]\n",
    "    # create arry from forecasts and reshape [number of forecasts, forecast_period]\n",
    "    yhat = np.array(forecasts)\n",
    "    yhat = yhat.reshape(len(yhat), n_seq)\n",
    "    # concat X_test with forecasts and train with test (incl. forecasts)\n",
    "    test_re = np.concatenate((X_test, yhat), axis=1)\n",
    "    reframed_yhat = np.concatenate((train, test_re))\n",
    "    # inverse scale reframed dataset with forecasts\n",
    "    inv_scale_df = scaler.inverse_transform(reframed_yhat)\n",
    "    # select only forecasts --> now unscaled, but still differenced\n",
    "    inv_scale_test = inv_scale_df[-n_test:]\n",
    "    forecasts = [row[-forecast_period:] for row in inv_scale_test]\n",
    "    # create list for storing undifferenced values\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        inv_scale = np.array(forecasts[i])\n",
    "        # invert differencing\n",
    "        index = len(df) - (n_test+(n_seq-1)) + i - 1\n",
    "        last_ob = df.values[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted, forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forecasts(actual, forecasts, n_lag, n_seq, n_features):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. actual: y test values (unscaled and undifferenced)\n",
    "        2. forecasts: forecasted y values (unscaled and undifferenced)\n",
    "        3. n_lag: number of lag days that are used for forecasting (=lag_days)\n",
    "        4. n_seq: number of forecasting days (=forecast_period)\n",
    "        5. n_features: Number of exogenous features used\n",
    "    Returns: \n",
    "        1. RMSE score\n",
    "        2. Adj. R^2 score\n",
    "        3. MAE score\n",
    "    \"\"\"\n",
    "    rmse_pred = list()\n",
    "    R2_pred = list()\n",
    "    mae_pred = list()\n",
    "    for i in range(n_seq):\n",
    "        acutal = [row[i] for row in actual]\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        rmse = sqrt(mean_squared_error(acutal, predicted))\n",
    "        rmse_pred.append(rmse)\n",
    "        R2_score = 1-(1-r2_score(acutal, predicted))*(len(forecasts)-1)/(len(forecasts) - \n",
    "                                                                         n_features - 1)\n",
    "        R2_pred.append(R2_score)\n",
    "        mae_score = mean_absolute_error(acutal, predicted)\n",
    "        mae_pred.append(mae_score)\n",
    "        #print('t+%d RMSE: %f' % ((i+1), rmse)) # optionally show\n",
    "    return rmse_pred, R2_pred, mae_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_RMSE(df,actual, forecasts, train, test, scaler, n_test, n_lag, n_seq, n_features):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        1. df: Pandas dataframe for debugging (unscaled and undifferenced)\n",
    "        2. actual: y test values (scaled and differenced)\n",
    "        3. forecasts: forecasted y values (scaled and differenced)\n",
    "        4. train: train dataset (scaled and differenced)\n",
    "        5. test: test dataset (scaled and differenced)\n",
    "        6. scaler: the scaler used (e.g. MinMaxScaler)\n",
    "        7. n_test: size of test set as an absolute number\n",
    "        3. n_lag: number of lag days that are used for forecasting (=lag_days)\n",
    "        4. n_seq: number of forecasting days (=forecast_period)\n",
    "        10. n_features: Number of exogenous features used\n",
    "    Returns: \n",
    "        1. RMSE score\n",
    "        2. Adj. R^2 score\n",
    "        3. MAE score\n",
    "        4. forecasts(unscaled und undifferenced)\n",
    "        5. actual(unscaled and undifferenced)\n",
    "    \"\"\"\n",
    "    forecasts_inv, forecasts = inverse_transform(df=df, forecasts=forecasts, train=train, \n",
    "        test=test, scaler=scaler, n_test=n_test,n_lag=n_lag, n_seq=n_seq, n_features=n_features)\n",
    "    actual_inv, actual = inverse_transform(df=df, forecasts=actual, train=train, test=test, \n",
    "        scaler=scaler, n_test=n_test, n_lag=n_lag, n_seq=n_seq, n_features=n_features)\n",
    "    rmse_pred, R2_pred, mae_pred = evaluate_forecasts(actual=actual_inv, \n",
    "        forecasts=forecasts_inv, n_lag=n_lag, n_seq=n_seq, n_features=n_features)\n",
    "    return rmse_pred, R2_pred, mae_pred, forecasts_inv, actual_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
